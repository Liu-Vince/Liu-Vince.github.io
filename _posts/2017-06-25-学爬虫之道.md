---
layout: post
title:  "学爬虫之道"
date:   2017-06-25 1:05:52 +800
categories: [2017年]
tags: [Python, 网络爬虫]
comments: 1
---
近来在阅读 《轻量级 Django》,虽然还没有读完，但我已经收益颇多。我不得不称赞 Django 框架的开发人员，他们把 Web 开发降低门槛。Django 让我从对 Web 开发是一无所知到现在可以编写小型 web 应用，这很舒服。

Django 已经算是入门，所以自己把学习目标转到爬虫。自己接下来会利用三个月的时间来专攻 Python 爬虫。这几天，我使用“主题阅读方法”阅读 Python 爬虫入门的文档。制定 Python 爬虫的学习路线。

### 第一阶段：夯实
入门要就是在打基础，所以要从最基础的库学起。下面是几个库是入门最经典的库
#### 1）urllib 
它属于 Python 标准库。该库的作用是请求网页并下载数据。在学习该库之前，最好把 HTTP 协议了解下。这会大大提高后面的学习效率。

先学会如何使用 urllib 请求到数据，再学习一些高级用法。例如：
- 设置 Headers: 某些网站反感爬虫的到访，于是对爬虫一律拒绝请求。设置 Headers 可以把请求伪装成浏览器访问网站。
- Proxy 的设置: 某些站点做了反倒链的设置，会将高频繁访问的 IP 地址封掉。所以我们需要用到代理池。
- 错误解析：根据 URLError 与 HTTPError 返回的错误码进行解析。
- Cookie 的使用：可以模拟网站登录，需要结合 cookielib 一起使用。

#### 2）re
re 是正则表达式库。同时也是 Python 标准库之一。它的作用是匹配我们需要爬取的内容。所以我们需要掌握正则表达式常用符号以及常用方法的用法。

#### 3）BeautifulSoup
BeautifulSoup 是解析网页的一款神器。它可以从 HTML 或者 XML 文件中提取数据。配合 urllib 可以编写出各种小巧精干的爬虫脚本。

### 第二阶段：进阶
当把基础打牢固之后，我们需要更进一步学习。使用更加完善的库来提高爬取效率
#### 1) 使用多线程
使用多线程抓取数据，提高爬取数据效率。
#### 2）学习 Requests
Requests 作为 urlilb 的替代品。它是更加人性化、更加成熟的第三方库。使用 Requests 来处理各种类型的请求，重复抓取问题、cookies 跟随问题、多线程多进程、多节点抓取、抓取调度、资源压缩等一系列问题。
#### 3）学习 Xpath
Xpath 也算是一款神器。它是一款高效的、表达清晰简单的分析语言。掌握它以后介意弃用正则表达式了。一般是使用浏览器的开发者工具 加 lxml 库。
#### 4）学习 Selenium
使用 Selenium，模拟浏览器提交类似用户的操作，处理js动态产生的网页。因为一些网站的数据是动态加载的。类似这样的网站，当你使用鼠标往下滚动时，会自动加载新的网站。

### 第三阶段：突破
#### 1）学习 Scrapy
Scrapy 是一个功能非常强大的分布式爬虫框架。我们学会它，就可以不用重复造轮子。

#### 2）数据存储
如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySql
MongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。

### 第四阶段：为我所用
当爬虫完成工作，我们已经拿到数据。我们可以利用这些数据做数据分析、数据可视化、做创业项目原始启动数据等。
我们可以学习 NumPy、Pandas、 Matplotlib 这三个库。
- NumPy ：它是高性能科学计算和数据分析的基础包。
- Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。
- Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。

> 本文原创发布于微信公众号「极客猴」，欢迎关注第一时间获取更多原创分享